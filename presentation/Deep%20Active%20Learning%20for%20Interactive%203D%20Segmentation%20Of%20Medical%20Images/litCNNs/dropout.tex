Dropout is another method used to minimize overfitting \cite{Dropout}. Random neurons will "drop out" of the training process, that is to say all weights into the next layer are temporarily set to 0 and frozen such that they are not updated with backpropagation. This stops the network from over-relying on certain neurons (features) by pulling them out of the training process, thus forcing the network to rely on other neurons (features), making it more generalizable to cases when the original neuron (feature) is weak. Dropout has the effect of training multiple sub-networks within the network that are composed of different feature sets. The probability that a certain neuron will drop out (the dropout rate) is usually set between 0.4 and 0.6. When adding such a high dropout rate to a CNN, it is often necessary to increase the number of neurons per layer to maximize performance. 

For some time it was thought that dropout in CNNs would at best cause a needless increase in memory and decrease in training speed. The reasoning is that pixels randomly dropped out of feature maps are likely to have highly correlated features nearby which haven't been dropped out, which means the dropout will have little effect. However, further work showed that spatial dropouts, where entire feature channels are dropped out rather than individual pixels, is an effective dropout method for CNNs \cite{SpatialDropout}.