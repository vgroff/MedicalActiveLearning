Transfer learning is when the weights of a trained CNN are transfered into a new CNN working on a different problem. The new CNN is then trained, during which time its weights are fine-tuned to the problem at hand. Tajbaksh et al. found that a fine-tuned CNN performs at least as well as a fully-trained one on medial imagery, often outperforming it \cite{fineTune}. The pre-trained CNN is one which works on normal (non-medical) imagery. The intuition is that the lower (convolutional) levels of the CNN will work to extract general spatial features, which become more specialized to the problem at hand deeper into the network. Large parts of the network can thus be re-used for the new problem, which makes the training time much shorter. Tajbaksh et al. therefore suggest fine-tuning by only training the later layers of the CNN, and freezing out the earlier layers. Transfer learning also tends to reduce overfitting, if the original CNN is well-trained then the features it extracts should be generalizable (for example, spatially speaking) and the new CNN will therefore generalise better. 