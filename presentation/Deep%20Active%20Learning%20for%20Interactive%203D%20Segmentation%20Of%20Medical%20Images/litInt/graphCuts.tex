There is a more standard way of solving the CRF equation. It can be proven that by formulating the problem as a maxflow problem, a minimum cost cut will find the global minimum of the cost function in polynomial time, segmenting the image into 2 regions.
The image is converted to a graph data structure $G=(V,E)$, with $n$ vertices for each pixel, one as a source terminal representing the object and the other as the sink terminal representing the background. The weight of the edges between pixels is then the value of the boundary function. The weight of the edges between a pixel and either terminal binary terminal $k$ is $\lambda R_{i,j,\neg k}$ with $R_{i,j,\neg k}$ the regional term with binary label $\neg k$ for pixel $(i,j)$ where $\neg$ is the logical not operator. That is to say for a given pixel, a penalty for the background becomes an edge between the pixel and the terminal representing the object, and vice-versa. It can be proven that a minimum cost cut on the graph will now find the global minimum of the cost function in polynomial time, segmenting the images into 2 regions, connected to either terminal, representing the object and the background \cite{graphCuts} \cite{minCut}. 

A minimum cost cut is the smallest total weight of edges which, if removed, would disconnect the sink from the source. The max-flow min-cut theorem states that finding a minimum cost cut and finding the maximum flow from source to sink in a network where the edge weights are flow capacities are mathematically identical problems. The edges who's flow capacity is saturated from the maximal flow are the edges which are part of the minimal cut. Path based algorithms work by doing a breadth first search for the shortest non-saturated path from source to sink, and then increasing the flow until one of the edges along said path is saturated. The process is then repeated until flow can no longer be increased. Boykov and Kolmogorov make empirical comparisons of various max-flow algorithms, and develop one which empirically outperforms the state of the art at the time (BK algorithm) \cite{minCut}. The path algorithms would start a new breadth-first-search once all the paths of a given length are exhausted, a process which is expensive. The BK algorithm relies on re-using search trees for previous breadth-first-searches to improve empirical performance. 

Since then, improvements have been made to the BK algorithm. The algorithm has been parallelized by the work of Liu and Sun \cite{BottomUpGraphCuts}, by splitting the original graph into sub-graphs with sub-paths each calculated by a different CPU core, which are then combined. Jamriska and Sykora have managed to achieve large reductions in computational time for grid-like graphs (as is the case with images) by efficiently holding the graphs in memory\cite{GridCuts}. Namely vertices are held in memory with their neighbouring vertices close by. Since the CPU cache loads in 64 byte blocks it will load vertices nearby to the vertex it is trying to load, such that the next vertices are already directly accessible from the cache when needed. 
