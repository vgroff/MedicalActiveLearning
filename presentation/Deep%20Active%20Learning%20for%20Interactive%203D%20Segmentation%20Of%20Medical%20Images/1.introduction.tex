\section{Introduction}

Various forms of AI can greatly improve the productivity of human labour. Visual recognition is part of most of the work that humans do, and some work relies heavily on it. Work done by doctors in medical imaging, such as organ and tumour segmentations in MRI or CT scans, could be made much more efficient with the introduction of image recognition AI. Unfortunately many forms of AI suffer various limitations, including difficulty in performing accurate predictions on types of object (e.g. types of tumours) that they have never seen before. This paper provides a framework implemented as a website, which aims to:
\begin{itemize}
\item Perform fast and accurate segmentations on 3D medical images on both seen and unseen objects with the help of user interaction
\item Offer the ability for the AI to learn from the unseen objects once they have been segmented
\end{itemize}

There have been significant advances in image recognition technology in recent years, namely through the use of convolutional neural networks (CNNs), an algorithm for statistical machine learning. Convolutional operations are highly parallelisable, and thus the harnessing of GPUs has allowed for increasingly complex networks. Most famously, Cirescan et al. \cite{cirescan} in 2012 reported a 30-80\% reduction in error rates over state of the art techniques of the time. The CIFAR-10 dataset contains 60,000 images classified into 10 classes, ranging from planes to frogs. It is a difficult dataset, since the image may show only part of the object and the backgrounds can vary wildly, but Cirescan et al. CNN reached a classification accuracy of 88.79\%. Since then, various frameworks that use CNNs have been developed to tackle the problem of medical image segmentation which outperform the traditional techniques \cite{fcn, uNet, UI-Net, deepIGeoS, BIFSeg}. 


Despite their high levels of accuracy, there are limitations to what CNNs can achieve in the context of medical imaging. Firstly, CNNs need large datasets, which need to be correspondingly labelled. In the case of planes or frogs, the data is widely available and labeling is low skilled work and thus can be achieved rather cheaply. Medical data however, is less easily available and requires expertise to label. The second issue is that CNNs are generally poorly adaptive to large variations in the test data, which would be expected when it is used in practice on new (different) patients or different imaging equipment. For these reasons, we propose to pursue an interactive method, where the user can correct any mistakes made by the CNN, and the CNN can improve its guess using the correction. This should still result in massive efficiency gains for the doctors without handing too much decision-making power over to an insufficiently trained machine. 

For segmentations, we produce a website based on the interactive CNN framework of Wang et al. \cite{BIFSeg} with various modifications. To deal with unseen objects, we train a CNN on a variety of different organs so that it learns generalizable features rather than features specific to a certain organ. As the user produces segmentations, they can upload these to the website. The user can then train a CNN on the uploaded images, so that it may perform better for the task at hand in the future. Here the CNN is trained through transfer learning, which improves training speed and often improves performance as well \cite{fineTune}. User interactivity demands fast inference, and not all local machines have GPU technology capable of achieving this. For this reason, we propose hosting the CNN on the web server with a powerful GPU rather than running it on the users machine.  
